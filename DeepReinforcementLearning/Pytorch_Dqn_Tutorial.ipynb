{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Auto Differentiation Package\n",
    "\n",
    "- **Pytorch supports reverse-mode auto-differentiation**\n",
    "\n",
    "- Dynamic execution / graph construction **(CG)** (Unlike TF or Caffe)\n",
    "\n",
    "- No need for forward graph\n",
    "\n",
    "- Allows in-place operations\n",
    "\n",
    "- Disjoint graphs. Only include relevant subset of the CG\n",
    "\n",
    "- - -\n",
    "\n",
    "### Eager execution\n",
    "\n",
    "- Red squares: Tensors\n",
    "\n",
    "- Gray circles: Operations\n",
    "\n",
    "- - -\n",
    "\n",
    "- **Forward**\n",
    "<img src=\"CG.png\" width=\"400\">\n",
    "\n",
    "- **Backward**\n",
    "\n",
    "<img src=\"CG.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "B = 10\n",
    "\n",
    "# Allow grad flow\n",
    "x = torch.randn(B, 10, requires_grad=True)\n",
    "# No grad flow to y\n",
    "y = torch.randn(B, 5)\n",
    "\n",
    "def f(x, y):\n",
    "    z = x @ y    # Matmul          : (10x10),(10x5) -> (10x5)\n",
    "    t = z ** 2   # Power           : (10x5) -> (10x5)\n",
    "    z = t + z    # Elementwise sum : (10x5),(10x5) -> (10x5)\n",
    "\n",
    "    z = z.sum(1) # Sum at axis 1   : (10x5) -> (10)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient of a scalar output or ...\n",
    "\n",
    "Pytorch allows Jacobian vector product but it doesn't calculate Jacobian of $f$.\n",
    "\n",
    "If the output dimension is just the batch dimension, where we need to accumulate, this is not a problem. This way of auto-differentiation(AD) is very efficient and sufficient(most of the time) in **DL**.\n",
    "\n",
    "$$ f(x, y): R^{10x10}xR^{10x5} \\rightarrow R^{10} $$\n",
    "\n",
    "#### Jacobian Vector Product (jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector in jvp\n",
    "vectr_in_jvp = torch.ones(B)\n",
    "\n",
    "# Jacobian Vector Product calculation\n",
    "f(x, y).backward(vectr_in_jvp)\n",
    "\n",
    "# Storing the gradient\n",
    "jacob_vec_prod_grad = x.grad.clone()\n",
    "x.grad.zero_()\n",
    "\n",
    "# Output size of function $f$\n",
    "f(x, y).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient(differentiation in scalar output functions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f(.).sum() := scalar output function\n",
    "scalar_out = f(x, y).sum()\n",
    "scalar_out.backward()\n",
    "\n",
    "# Storing the gradient\n",
    "scalar_out_grad = x.grad.clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(jacob_vec_prod_grad == scalar_out_grad).all().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "- torch.nn\n",
    "- torch.optim\n",
    "\n",
    "Pytorch has a rich function and tool set for Neural networks.\n",
    "\n",
    "The core component is ```torch.nn.Module```.\n",
    "\n",
    "Please check: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/module.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\" Simple fully connected neural network with \n",
    "    Layer normalization layers.\n",
    "    Arguments:\n",
    "        - in_size: Input size\n",
    "        - out_size: Output size\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_size, out_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = torch.nn.Linear(in_size, 64)\n",
    "        self.nrm1 = torch.nn.LayerNorm(64)\n",
    "        self.rel1 = torch.nn.ReLU()\n",
    "        self.lin2 = torch.nn.Linear(64, 64)\n",
    "        self.nrm2 = torch.nn.LayerNorm(64)\n",
    "        self.rel2 = torch.nn.ReLU()\n",
    "        self.lin3 = torch.nn.Linear(64, out_size)\n",
    "        \n",
    "        self.layers = (self.lin1, self.nrm1, self.rel1, self.lin2, self.nrm2, self.rel2, self.lin3)\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        \n",
    "#         self.param_init()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" Forward function of the network.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def param_init(module):\n",
    "        \"\"\" Parameter initialization with relu gain.\n",
    "        \"\"\"\n",
    "        # Obtain gain for xavier initializer\n",
    "        gain = torch.nn.init.calculate_gain(\"relu\")\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            torch.nn.init.xavier_uniform_(module.weight, gain)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "When the super class is ```torch.nn.Module```, any attribute initiated within the class is registered according to:\n",
    "\n",
    "```\n",
    "def __setattr__(self, name, value):\n",
    "    ...\n",
    "```\n",
    "\n",
    "This method simply checks if:\n",
    "\n",
    "- Attribute is a ```Module``` object.\n",
    "\n",
    "    If so, it is registered to the ```_modules``` dict.\n",
    "\n",
    "- Attribute is a ```Parameter``` object\n",
    "\n",
    "    It is registered into ```_parameters``` dict.\n",
    "\n",
    "- - -\n",
    "\n",
    "This is how pytorch knows all the modules and parameters within the class(recursively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(4, 2)\n",
    "\n",
    "# All the modules registered during initialization\n",
    "net._modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```apply``` function is used to apply a function, that takes one input(module), recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_before = net.lin1.weight.std()\n",
    "# Apply initialization to all modules(recursively)\n",
    "net.apply(net.param_init)\n",
    "std_after = net.lin1.weight.std()\n",
    "\n",
    "# Change the device of all the modules(recursively) in the \"net\" object\n",
    "net.apply(lambda m: m.cuda())\n",
    "net.apply(lambda m: m.cpu())\n",
    "std_before, std_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward\n",
    "\n",
    "When the ```Module``` class instance(net in our example) is called, python calls ```__call__()``` method. Pytorch ```Module``` objects call the ```forward``` method in the ```__call__``` method which is overridden by the super class(Module). But it also checks and calls followings:\n",
    "\n",
    "- If there is a hook registered, the callback function is called with necessary arguments\n",
    "\n",
    "    - backward_hook: Called at the backward pass\n",
    "    - forward_hook: Called at the end of the forward pass\n",
    "    - forward_pre_hook: Called at the beginning of the forward pass\n",
    "    \n",
    "- Call and return ```forward```\n",
    "\n",
    "**Note**: Hooks are cool and useful but we will not mention them in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.rand(B, 4)\n",
    "output = net(input)           # __call__ is called with all the checks and hook functionality. (safer)\n",
    "output = net.forward(input)   # only forward is called\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN\n",
    "\n",
    "Vanilla DQN implementation. The loss we will be minimizing is semi-gradient loss given below.\n",
    "\n",
    "$$\n",
    "    TD = || \\max_{a_i} Q(s', a_i: \\theta') + r - Q(s, a: \\theta)  ||_{2}^{2}\n",
    "$$\n",
    "\n",
    "It is called **semi**-gradient since we do not take the derivative w.r.t target network as well. Instead, we use another network(seperate parameters $\\theta'$) to find the target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from collections import namedtuple\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print({\n",
    "    \"torch\": torch.__version__,\n",
    "    \"numpy\": np.__version__,\n",
    "    \"gym\": gym.__version__\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a replay buffer of transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample as randsample\n",
    "\n",
    "Transition = namedtuple(\"Transition\", (\"state\",\n",
    "                                        \"action\",\n",
    "                                        \"reward\",\n",
    "                                        \"next_state\",\n",
    "                                        \"terminal\")\n",
    "                                    )\n",
    "\n",
    "class UniformBuffer(object):\n",
    "    \"\"\" Queue implementation with lists that allows random\n",
    "    sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.queue = []\n",
    "        self.cycle = 0\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queue)\n",
    "\n",
    "    def push(self, **transition):\n",
    "        \"\"\" Push the transition keyword arguments into queue.\n",
    "        If list is full, then overwrite the oldest transition. Else,\n",
    "        append the transition.\n",
    "        \"\"\"\n",
    "        if self.size != len(self.queue):\n",
    "            self.queue.append(Transition(**transition))\n",
    "        else:\n",
    "            self.queue.append(Transition(**transition))\n",
    "            self.cycle  = (self.cycle + 1)%self.size\n",
    "    \n",
    "    def sample(self, batchsize):\n",
    "        \"\"\" Take a sample from the queue. If batchsize is larger than\n",
    "        the queue itself than return None.\n",
    "        \"\"\"\n",
    "        if batchsize > len(self.queue):\n",
    "            return None\n",
    "        batch = randsample(self.queue, batchsize)\n",
    "        return Transition(*zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dqn(torch.nn.Module):\n",
    "    \"\"\" Dqn implementation.\n",
    "    Arguments:\n",
    "        - valuenet: Neural network(torch.nn.Module) that will be used\n",
    "        for value estimation.\n",
    "        - buffersize: Size of the replay buffer.\n",
    "    \"\"\"\n",
    "    def __init__(self, valuenet, buffersize=10000):\n",
    "        super().__init__()\n",
    "        self.buffer = UniformBuffer(buffersize)\n",
    "        self.network = valuenet\n",
    "        self.targetnet = deepcopy(valuenet)\n",
    "        self.device = \"cpu\"\n",
    "        self.update_count = 0\n",
    "\n",
    "    def act(self, state, epsilon=0.0):\n",
    "        \"\"\" Epsilon delta policy.\n",
    "        Arguments:\n",
    "            - state: Batch size 1 torch tensor.\n",
    "            - epsilon: Random action probability.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            values = self.network(state)\n",
    "        values = values.squeeze()\n",
    "        if len(values.shape) > 1:\n",
    "            raise ValueError(\"Batch size of the given stat tensor for act must be 1!\")\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = torch.randint(values.shape[-1], (1,)).long()\n",
    "            value = values[action]\n",
    "        else:\n",
    "            action = torch.argmax(values)\n",
    "            value = values[action]\n",
    "        return (action.item(), value.item())\n",
    "        \n",
    "    def td_loss(self, gamma, batchsize):\n",
    "        \"\"\" TD loss of the semi-gradient Q learning.\n",
    "        Take a sample from the replay memory of size \"batchsize\".\n",
    "        Arguments:\n",
    "            - gamma: Discount factor\n",
    "            - batchsize: Batch size\n",
    "        \"\"\"\n",
    "        # Take a sample of transitions(iid assumption holds)\n",
    "        batch = self.buffer.sample(batchsize)\n",
    "        batch = self._batchtotorch(batch)\n",
    "\n",
    "        # Values of the next state are taken so that the gradient flow is \n",
    "        # disabled in\"next_values\". Because we use Semi-gradient q learning\n",
    "        # (a.k.a. Q Learning in function approximation).\n",
    "        with torch.no_grad():\n",
    "            next_values = self.targetnet(batch.next_state)\n",
    "            next_values = torch.max(next_values, dim=1, keepdim=True)[0]\n",
    "\n",
    "        current_values = self.network(batch.state)\n",
    "        current_values = current_values.gather(1, batch.action)\n",
    "\n",
    "        target_value = next_values*(1 - batch.terminal)*gamma + batch.reward\n",
    "        # We could use l2 loss or l1 loss as well. Remember that, the choice is \n",
    "        # environment and algorithm dependent.\n",
    "        td_error = torch.nn.functional.smooth_l1_loss(current_values, target_value)\n",
    "\n",
    "        return td_error\n",
    "\n",
    "    def update(self, opt, gamma, batchsize, target_update_period, grad_clip=False):\n",
    "        \"\"\" Update one step with the optimizer.\n",
    "        \"\"\"\n",
    "\n",
    "        # Update target network at every \"target_update_period\"\n",
    "        if self.update_count >= target_update_period:\n",
    "            self.update_count = 0\n",
    "            self.targetnet.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss = self.td_loss(gamma, batchsize)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the gradients to prevent gradients to diverge\n",
    "        if grad_clip:\n",
    "            for param in self.network.parameters():\n",
    "                param.grad.data.clamp_(-1, 1)\n",
    "            \n",
    "        opt.step()\n",
    "        self.update_count += 1\n",
    "        return loss.item()\n",
    "\n",
    "    def push(self, state, action, reward, next_state, terminal):\n",
    "        self.buffer.push(**dict(state=state,\n",
    "                                action=action,\n",
    "                                reward=reward,\n",
    "                                next_state=next_state,\n",
    "                                terminal=terminal))\n",
    "\n",
    "    def _batchtotorch(self, batch):\n",
    "        \"\"\" Helper method to convert raw batch into tensors.\n",
    "        \"\"\"\n",
    "        state = self._totorch(batch.state, torch.float32)\n",
    "        action = self._totorch(batch.action, torch.long).view(-1, 1)\n",
    "        next_state = self._totorch(batch.next_state, torch.float32)\n",
    "        terminal = self._totorch(batch.terminal, torch.float32).view(-1, 1)\n",
    "        reward = self._totorch(batch.reward, torch.float32).view(-1, 1)\n",
    "        return Transition(state, action, reward, next_state, terminal)\n",
    "\n",
    "    def _totorch(self, container, dtype):\n",
    "        tensor = torch.tensor(container, dtype=dtype)\n",
    "        return tensor.to(self.device)\n",
    "\n",
    "    def to(self, device):\n",
    "        super().to(device)\n",
    "        self.device = device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a DQN agent in LunarLander"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, net, opt, env):\n",
    "    # Convert the model into given \"device\"\n",
    "    agent.to(args.device)\n",
    "    \n",
    "    # List of the rewards\n",
    "    reward_list = []\n",
    "\n",
    "    for eps in range(args.episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        # Decay the epsilon so that the policy can converge\n",
    "        eps_reward = 0\n",
    "        episode_value = 0\n",
    "        epsilon = (args.start_epsilon-args.end_epsilon)*(1-eps/args.episodes) + args.end_epsilon\n",
    "        \n",
    "        for i in range(args.maxlength):\n",
    "            torch_state = agent._totorch(state, torch.float32).view(1, -1)\n",
    "            action, value = agent.act(torch_state, epsilon)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Gather statistics\n",
    "            eps_reward += reward\n",
    "            episode_value += value\n",
    "\n",
    "            # Start updating when the replay memory has enough samples\n",
    "            if len(agent.buffer) > args.batchsize:\n",
    "                td_loss = agent.update(opt, args.gamma, args.batchsize, args.target_period, args.clipgrad)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # End of a trajectory\n",
    "        reward_list.append(eps_reward)\n",
    "        print(\"Episode: {}, avg epsiodic reward: {}\".format(eps, np.mean(reward_list[-10:])))\n",
    "    return reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can play with the hyperparameters\n",
    "hyperparams = {\n",
    "    \"lr\": 0.001,\n",
    "    \"gamma\": 0.98,\n",
    "    \"episodes\": 300,\n",
    "    \"buffersize\": 10000,\n",
    "    \"target_period\": 50,\n",
    "    \"batchsize\": 128,\n",
    "    \"start_epsilon\": 0.9,\n",
    "    \"end_epsilon\": 0.1,\n",
    "    \"maxlength\": 300,\n",
    "    \"clipgrad\": True,\n",
    "    \"device\": \"cpu\",\n",
    "    \"seed\": 12020,    # Holocene year(current year + 10000)\n",
    "}\n",
    "\n",
    "\n",
    "# Use namedtuple instead of a dictionary(ofcourse optional)\n",
    "HyperParams = namedtuple(\"HyperParams\", hyperparams.keys())\n",
    "args = HyperParams(**hyperparams)\n",
    "\n",
    "# Initiate enviroment\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "insize = env.observation_space.shape[0]\n",
    "outsize = env.action_space.n\n",
    "\n",
    "# Seed\n",
    "torch.manual_seed(args.seed)\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "env.seed(args.seed)\n",
    "\n",
    "# Initiate agent and optimizer\n",
    "net = Net(insize, outsize)\n",
    "agent = Dqn(net, buffersize=args.buffersize)\n",
    "opt = torch.optim.Adam(agent.parameters(), args.lr)\n",
    "\n",
    "\n",
    "# Start training\n",
    "rewards = train(args, net, opt, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_trajectory(args, agent, env):\n",
    "    \"\"\" Single trajectory evaluation of the agent with rendering.\n",
    "    \"\"\"\n",
    "    # Seed\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    env.seed(args.seed)\n",
    "\n",
    "    reward = 0\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        torch_state = agent._totorch(state, torch.float32).view(1, -1)\n",
    "        action, value = agent.act(torch_state, args.end_epsilon)\n",
    "        state, r, done, _ = env.step(action)\n",
    "        reward += r\n",
    "        env.render()\n",
    "        time.sleep(1/30)\n",
    "    return reward\n",
    "\n",
    "\"Reward :\", run_trajectory(args, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot episodic rewards obtained durin training(average of last 10 episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(np.convolve(rewards, np.ones(5)/5))\n",
    "plt.ylabel(\"Episodic reward\")\n",
    "plt.xlabel(\"episode\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.state_dict(), \"LunarLander_dqn_agent.b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
